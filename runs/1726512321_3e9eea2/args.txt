run.py train_policy_with_original_rewards Hopper-v4 --n_envs 16 --million_timesteps 10
Namespace(batchnorm=False, debug=False, dropout=0.0, ent_coef=0.01, env='Hopper-v4', load_policy_ckpt_dir=None, load_prefs_dir=None, load_reward_predictor_ckpt_dir=None, log_dir=None, log_interval=100, lr=0.0007, lr_zero_million_timesteps=None, max_prefs=3000, max_segs=1000, million_timesteps=10.0, mode='train_policy_with_original_rewards', n_envs=16, n_initial_epochs=200, n_initial_prefs=500, policy_ckpt_interval=100, render_episodes=False, reward_predictor_ckpt_interval=1, reward_predictor_learning_rate=0.0002, run_name='1726512321', seed=0, synthetic_prefs=False, test_mode=False)